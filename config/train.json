{   
    "seed": 1234,
    "lr": 5e-4,
    "n_epochs": 20,
    "batch_size": 8,
    "max_input_length": 512,
    "max_output_length": 512,
    "weight_decay": 0.01,
    "gradient_accumulation_steps": 4,
    "lora": {
        "r": 32,
        "alpha": 32,
        "target_modules": ["q", "v"],
        "dropout": 0.05,
        "bias": "none"
    },
    "output_dir": "output/gpt2",
    "evaluation_strategy": "steps",
    "eval_steps": 500,
    "save_strategy": "steps",
    "save_steps": 500,
    "logging_strategy": "steps",
    "logging_steps": 500,
    "report_to": "tensorboard",
    "save_model_dir": "results/gpt2",
    "logging_dir": "logs",
    "early_stopping_patience": 3,
    "metrics": ["bleu", "rouge"]
}

