2024-01-03 22:15:54,930 INFO: {'model_name': 'google/flan-t5-base', 'tokenizer_name': 'google/flan-t5-base'} 
2024-01-03 22:15:54,931 INFO: {'seed': 114514, 'lr': 0.001, 'n_epochs': 20, 'batch_size': 8, 'max_input_length': 512, 'max_output_length': 32, 'weight_decay': 0.01, 'gradient_accumulation_steps': 4, 'lora': {'r': 32, 'alpha': 32, 'target_modules': ['q', 'v'], 'dropout': 0.05, 'bias': 'none'}, 'output_dir': 'output/512-32', 'evaluation_strategy': 'steps', 'eval_steps': 300, 'save_strategy': 'steps', 'save_steps': 300, 'logging_strategy': 'steps', 'logging_steps': 300, 'report_to': 'tensorboard', 'save_model_dir': 'results/512-32', 'logging_dir': 'logs', 'metrics': ['bleu', 'rouge', 'perplexity']} 
2024-01-03 22:15:59,233 INFO: Loaded bleu... 
2024-01-03 22:16:01,781 INFO: Loaded rouge... 
2024-01-03 22:16:03,926 INFO: Loaded perplexity... 
2024-01-03 22:16:06,848 INFO: Successfully loaded dataset 
2024-01-03 22:16:09,707 INFO: Train dataset size: 10191 
2024-01-03 22:16:09,707 INFO: Test dataset size: 2295 
2024-01-03 22:16:09,707 INFO: Valid dataset size: 2156 
2024-01-03 22:39:50,866 INFO: Computing bleu... 
2024-01-03 22:39:51,435 INFO: Computing rouge... 
2024-01-03 22:39:51,450 INFO: Using default tokenizer. 
2024-01-03 22:39:52,883 INFO: Computing perplexity... 
2024-01-03 22:40:04,402 INFO: bleu-2: 0.2706661273497541 
2024-01-03 22:40:04,402 INFO: rouge-L: 0.17180613695678665 
2024-01-03 22:40:04,402 INFO: perplexity: 19.07590371182323 
2024-01-03 23:14:28,840 INFO: Computing bleu... 
2024-01-03 23:14:29,248 INFO: Computing rouge... 
2024-01-03 23:14:29,261 INFO: Using default tokenizer. 
2024-01-03 23:14:30,585 INFO: Computing perplexity... 
2024-01-03 23:14:39,549 INFO: bleu-2: 0.3382469121529183 
2024-01-03 23:14:39,549 INFO: rouge-L: 0.19894944656637759 
2024-01-03 23:14:39,549 INFO: perplexity: 21.312733904992495 
2024-01-03 23:35:44,804 INFO: Computing bleu... 
2024-01-03 23:35:45,218 INFO: Computing rouge... 
2024-01-03 23:35:45,231 INFO: Using default tokenizer. 
2024-01-03 23:35:46,555 INFO: Computing perplexity... 
2024-01-03 23:35:54,730 INFO: bleu-2: 0.3985333268002679 
2024-01-03 23:35:54,730 INFO: rouge-L: 0.23858119680155063 
2024-01-03 23:35:54,730 INFO: perplexity: 19.982129391579992 
2024-01-03 23:57:04,382 INFO: Computing bleu... 
2024-01-03 23:57:04,780 INFO: Computing rouge... 
2024-01-03 23:57:04,792 INFO: Using default tokenizer. 
2024-01-03 23:57:06,110 INFO: Computing perplexity... 
2024-01-03 23:57:14,443 INFO: bleu-2: 0.35325023754960594 
2024-01-03 23:57:14,443 INFO: rouge-L: 0.20738195645143215 
2024-01-03 23:57:14,443 INFO: perplexity: 20.928027278638282 
2024-01-04 00:18:27,038 INFO: Computing bleu... 
2024-01-04 00:18:27,429 INFO: Computing rouge... 
2024-01-04 00:18:27,442 INFO: Using default tokenizer. 
2024-01-04 00:18:28,731 INFO: Computing perplexity... 
2024-01-04 00:18:37,473 INFO: bleu-2: 0.3686951454896391 
2024-01-04 00:18:37,473 INFO: rouge-L: 0.217325692764048 
2024-01-04 00:18:37,473 INFO: perplexity: 21.489023269660397 
